{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_BYlSg9lfDj",
        "outputId": "8839e085-613a-4181-a1b7-914d0964d9ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git\n",
            "  Cloning https://github.com/MeMartijn/updated-sklearn-crfsuite.git to /tmp/pip-req-build-r3d08th8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MeMartijn/updated-sklearn-crfsuite.git /tmp/pip-req-build-r3d08th8\n",
            "  Resolved https://github.com/MeMartijn/updated-sklearn-crfsuite.git to commit 675038761b4405f04691a83339d04903790e2b95\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6) (4.66.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6) (0.9.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn-crfsuite\n",
            "  Building wheel for sklearn-crfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn-crfsuite: filename=sklearn_crfsuite-0.3.6-py2.py3-none-any.whl size=10866 sha256=a7a6dda5b7da110fbe2d7899b25650bdf4b2b4033746498eb82d086bde4c6ffe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gf_f78sv/wheels/0b/bc/07/bd75a6f5fa2bf2ea05a5aad8d9ac66d2b5aab93dfd4e1a89de\n",
            "Successfully built sklearn-crfsuite\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.10 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re_C6i76lZc4",
        "outputId": "ede208ab-fea4-4b66-950a-6f806a334c6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.2.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: bigframes, fastai, imbalanced-learn, librosa, mlxtend, qudida, sklearn-pandas, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5gdrh7Z_Ic",
        "outputId": "0d67a8e5-a977-446a-f364-0aa96af712e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "penn_treebank = []\n",
        "for fileid in treebank.fileids():\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for word, tag in treebank.tagged_words(fileid):\n",
        "    tokens.append(word)\n",
        "    tags.append(tag)\n",
        "  penn_treebank.append((tokens, tags))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwZYkNr1bPN",
        "outputId": "f6358072-f084-4332-d0eb-11438b525578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install conllu\n",
        "! wget https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master/es_gsd-ud-test.conllu\n",
        "\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "data_file = open(\"es_gsd-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
        "ud_files = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    ud_files.append(tokenlist)\n",
        "\n",
        "ud_treebank = []\n",
        "for sentence in ud_files:\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for token in sentence:\n",
        "    tokens.append(token['form'])\n",
        "    tags.append(token['upostag'])\n",
        "  ud_treebank.append((tokens, tags))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n",
            "--2024-03-30 16:12:27--  https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master/es_gsd-ud-test.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 737048 (720K) [text/plain]\n",
            "Saving to: ‘es_gsd-ud-test.conllu’\n",
            "\n",
            "es_gsd-ud-test.conl 100%[===================>] 719.77K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-03-30 16:12:27 (23.2 MB/s) - ‘es_gsd-ud-test.conllu’ saved [737048/737048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "source": [
        "import re\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "source": [
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "penn_train_size = int(0.8*len(penn_treebank))\n",
        "penn_training = penn_treebank[:penn_train_size]\n",
        "penn_testing = penn_treebank[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
        "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
        "\n",
        "ud_train_size = int(0.8*len(ud_treebank))\n",
        "ud_training = ud_treebank[:ud_train_size]\n",
        "ud_testing = ud_treebank[ud_train_size:]\n",
        "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
        "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHTkotyWpd28",
        "outputId": "1f1e132c-dadc-419f-de71-9b4317cb6c51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "penn_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on Penn Treebank corpus!\")\n",
        "penn_crf.fit(X_penn_train, y_penn_train)\n",
        "print(\"Finished training on Penn Treebank corpus!\")\n",
        "\n",
        "ud_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on UD corpus!\")\n",
        "ud_crf.fit(X_ud_train, y_ud_train)\n",
        "print(\"Finished training on UD corpus!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.66.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.10)\n",
            "Started training on Penn Treebank corpus!\n",
            "Finished training on Penn Treebank corpus!\n",
            "Started training on UD corpus!\n",
            "Finished training on UD corpus!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlJwNkF_0zs",
        "outputId": "283b402a-7e6c-435a-e15f-cff7bcccb895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "y_penn_pred=penn_crf.predict(X_penn_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
        "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
        "\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_true=y_penn_test, y_pred=y_penn_pred, labels=penn_crf.classes_, digits=3\n",
        "))\n",
        "\n",
        "print(\"## UD ##\")\n",
        "\n",
        "y_ud_pred=ud_crf.predict(X_ud_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_ud_test, y_ud_pred,average='weighted',labels=ud_crf.classes_))\n",
        "y_ud_pred_train=ud_crf.predict(X_ud_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_ud_train, y_ud_pred_train,average='weighted',labels=ud_crf.classes_))\n",
        "\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_ud_test, y_ud_pred, labels=ud_crf.classes_, digits=3\n",
        "))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Penn ##\n",
            "F1 score on Test Data\n",
            "0.9668646324625245\n",
            "F1 score on Training Data \n",
            "0.9936643188628935\n",
            "Class wise score:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NNP      0.952     0.963     0.957      1213\n",
            "           ,      1.000     1.000     1.000       592\n",
            "          CD      1.000     0.999     0.999       683\n",
            "         NNS      0.964     0.986     0.975       740\n",
            "          JJ      0.879     0.912     0.895       731\n",
            "          MD      0.993     1.000     0.996       135\n",
            "          VB      0.980     0.946     0.963       313\n",
            "          DT      0.992     0.993     0.992      1062\n",
            "          NN      0.962     0.955     0.958      1899\n",
            "          IN      0.981     0.980     0.981      1285\n",
            "           .      1.000     1.000     1.000       509\n",
            "         VBZ      0.958     0.936     0.947       219\n",
            "         VBG      0.936     0.876     0.905       185\n",
            "          CC      1.000     0.997     0.998       287\n",
            "         VBD      0.965     0.945     0.955       492\n",
            "         VBN      0.917     0.907     0.912       279\n",
            "      -NONE-      0.998     1.000     0.999       871\n",
            "          RB      0.912     0.912     0.912       296\n",
            "          TO      1.000     1.000     1.000       298\n",
            "         PRP      1.000     1.000     1.000       150\n",
            "         RBR      0.375     0.231     0.286        13\n",
            "         WDT      0.954     1.000     0.976        62\n",
            "         VBP      0.878     0.902     0.890       112\n",
            "          RP      0.667     0.720     0.692        25\n",
            "        PRP$      1.000     1.000     1.000        74\n",
            "         JJS      0.960     1.000     0.980        24\n",
            "         POS      0.992     1.000     0.996       124\n",
            "          ``      1.000     1.000     1.000        55\n",
            "          EX      0.750     1.000     0.857         3\n",
            "          ''      1.000     1.000     1.000        52\n",
            "          WP      1.000     1.000     1.000        13\n",
            "           :      1.000     1.000     1.000        49\n",
            "         JJR      0.764     0.894     0.824        47\n",
            "         WRB      1.000     0.955     0.977        22\n",
            "           $      1.000     1.000     1.000       170\n",
            "        NNPS      0.739     0.459     0.567        37\n",
            "         WP$      1.000     1.000     1.000         4\n",
            "       -LRB-      1.000     1.000     1.000        16\n",
            "       -RRB-      1.000     1.000     1.000        16\n",
            "         PDT      0.000     0.000     0.000         4\n",
            "         RBS      1.000     1.000     1.000         1\n",
            "          FW      0.000     0.000     0.000         0\n",
            "          UH      0.000     0.000     0.000         0\n",
            "         SYM      0.000     0.000     0.000         0\n",
            "          LS      0.000     0.000     0.000         0\n",
            "           #      0.000     0.000     0.000         0\n",
            "\n",
            "   micro avg      0.967     0.967     0.967     13162\n",
            "   macro avg      0.814     0.814     0.813     13162\n",
            "weighted avg      0.967     0.967     0.967     13162\n",
            "\n",
            "## UD ##\n",
            "F1 score on Test Data \n",
            "0.8658326606082186\n",
            "F1 score on Training Data \n",
            "0.9955278909422882\n",
            "Class wise score:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADP      0.967     0.967     0.967       242\n",
            "         ADV      0.859     0.802     0.830        91\n",
            "        VERB      0.822     0.804     0.813       189\n",
            "         DET      0.963     0.928     0.945       222\n",
            "        NOUN      0.854     0.844     0.849       326\n",
            "           _      0.913     0.875     0.894        24\n",
            "         ADJ      0.703     0.676     0.689       105\n",
            "       PROPN      0.395     0.870     0.543        54\n",
            "       PUNCT      0.984     1.000     0.992       183\n",
            "        PRON      0.929     0.793     0.855        82\n",
            "         AUX      0.868     0.819     0.843        72\n",
            "       CCONJ      0.953     0.897     0.924        68\n",
            "       SCONJ      0.867     0.912     0.889        57\n",
            "         NUM      1.000     0.931     0.964        29\n",
            "         SYM      1.000     0.571     0.727         7\n",
            "           X      0.000     0.000     0.000        18\n",
            "        INTJ      0.000     0.000     0.000         0\n",
            "\n",
            "   micro avg      0.864     0.865     0.865      1769\n",
            "   macro avg      0.769     0.746     0.748      1769\n",
            "weighted avg      0.874     0.865     0.866      1769\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4rcQq-ubbuT",
        "outputId": "e9909aef-5899-45c1-e93a-6e4300f3bf35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sent = \"The tagger produced good results\"\n",
        "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
        "\n",
        "penn_results = penn_crf.predict_single(features)\n",
        "ud_results = ud_crf.predict_single(features)\n",
        "\n",
        "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
        "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
        "\n",
        "print(penn_tups)\n",
        "print(ud_tups)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('tagger', 'NN'), ('produced', 'VBN'), ('good', 'JJ'), ('results', 'NNS')]\n",
            "[('The', 'DET'), ('tagger', 'NOUN'), ('produced', 'ADJ'), ('good', 'NOUN'), ('results', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2KlQu4c5-N"
      },
      "source": [
        "import pickle\n",
        "\n",
        "penn_filename = 'penn_treebank_crf_postagger.sav'\n",
        "pickle.dump(penn_crf, open(penn_filename, 'wb'))\n",
        "\n",
        "ud_filename = 'ud_crf_postagger.sav'\n",
        "pickle.dump(ud_crf, open(ud_filename,'wb'))"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}